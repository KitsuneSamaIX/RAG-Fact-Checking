{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizations",
   "id": "a1cdb9df94806f16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config",
   "id": "388f9600325c0188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%config InlineBackend.figure_format = 'svg'",
   "id": "67e77f7568e6a420",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe74be4a1ef34e71",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "id": "38f8e906293ce220",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aesthetics",
   "id": "f281fc14f2c785ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_theme(\n",
    "    style='whitegrid'\n",
    ")"
   ],
   "id": "37870ae7266d15c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "65faefa28c631317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.chdir('/Users/mattia/Documents/Projects/RAG-Fact-Checking/src')\n",
    "from src.reports import *"
   ],
   "id": "c549be661ddf9e57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "folder_path_prefix = '/Users/mattia/Desktop/Lab avanzato 1 - RAG/Results'",
   "id": "72402437e5277f52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "folder_name = '20241011-212456-UTC'\n",
    "file_name = 'metrics' + '.csv'\n",
    "input_file_path = os.path.join(folder_path_prefix, folder_name, file_name)\n",
    "metrics_df = pd.read_csv(input_file_path, index_col=0)"
   ],
   "id": "7e291b7f08f26a9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "folder_name = '20241011-212456-UTC'\n",
    "file_name = 'raw_data' + '.csv'\n",
    "input_file_path = os.path.join(folder_path_prefix, folder_name, file_name)\n",
    "raw_data_df = pd.read_csv(input_file_path, index_col=0)"
   ],
   "id": "283d8563b228a88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FIX DATA",
   "id": "2eb5541c0d87a884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fix_target(row: pd.Series) -> int:\n",
    "    if row['DATASET_NAME'] == 'climate_fever':\n",
    "        return 1 - row['target']\n",
    "    else:\n",
    "        return row['target']\n",
    "\n",
    "raw_data_df['target'] = raw_data_df.apply(fix_target, axis=1)"
   ],
   "id": "779949a385aeb9a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rb = ReportBuilderFor2ClassificationLevels()",
   "id": "1b42f6bba20fd2b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulate real run to rebuild the reports with the correct values\n",
    "\n",
    "# Initialize vars\n",
    "reports = []\n",
    "raw_data = []\n",
    "n_fixed = 0\n",
    "\n",
    "for llm_name in ['mistral-nemo:12b-instruct-2407-fp16', 'llama3.1:8b-instruct-fp16']:\n",
    "    # Update config params\n",
    "    config.LLM_NAME = llm_name\n",
    "    for dataset_name in ['climate_fever']:\n",
    "        # Update config params\n",
    "        config.DATASET_NAME = dataset_name\n",
    "        if dataset_name == 'climate_fever':  # Missing '.pqt' file for this dataset, so we use '.csv' instead\n",
    "            config.GROUND_TRUTH_DATASET_PATH = os.path.join(config.DATASET_PATH_PREFIX, config.DATASET_NAME, 'ground_truth.csv')\n",
    "        else:\n",
    "            config.GROUND_TRUTH_DATASET_PATH = os.path.join(config.DATASET_PATH_PREFIX, config.DATASET_NAME, 'ground_truth.pqt')\n",
    "        config.ALL_EVIDENCE_VECTOR_STORE_PATH = os.path.join(config.DATASET_PATH_PREFIX, config.DATASET_NAME, 'embeddings/512/')\n",
    "        for levels in [2, 6]:\n",
    "            if (levels == 6) and (dataset_name in ['climate_fever', 'feverous']):\n",
    "                continue  # These datasets do not support the 6 classification levels\n",
    "            # Update config params\n",
    "            config.CLASSIFICATION_LEVELS = levels\n",
    "            for fill, invert in [(True, False), (False, True), (False, False)]:\n",
    "                # Update config params\n",
    "                config.FILL_EVIDENCE = fill\n",
    "                config.FILL_EVIDENCE_UPPER_LIMIT = 10\n",
    "                config.INVERT_EVIDENCE = invert\n",
    "                for i in range(1, 11):\n",
    "                    # Update config params\n",
    "                    config.TRUNCATED_RANKING_RETRIEVER_RESULTS = i\n",
    "\n",
    "                    # Subset raw data\n",
    "                    raw_data_df_subset = raw_data_df[(raw_data_df.LLM_NAME == llm_name) & (raw_data_df.DATASET_NAME == dataset_name) & (raw_data_df.CLASSIFICATION_LEVELS == levels) & (raw_data_df.FILL_EVIDENCE == fill) & (raw_data_df.INVERT_EVIDENCE == invert) & (raw_data_df.TRUNCATED_RANKING_RETRIEVER_RESULTS == i)]\n",
    "                    \n",
    "                    # Subset metrics (only for assertion check)\n",
    "                    metrics_df_row = metrics_df[(metrics_df.LLM_NAME == llm_name) & (metrics_df.DATASET_NAME == dataset_name) & (metrics_df.CLASSIFICATION_LEVELS == levels) & (metrics_df.FILL_EVIDENCE == fill) & (metrics_df.INVERT_EVIDENCE == invert) & (metrics_df.TRUNCATED_RANKING_RETRIEVER_RESULTS == i)]\n",
    "                    \n",
    "                    # Compute correct values\n",
    "                    rb._predictions = raw_data_df_subset['prediction']\n",
    "                    rb._targets = raw_data_df_subset['target']\n",
    "                    rb_df_row = rb.build()\n",
    "                    \n",
    "                    # Replace values\n",
    "                    for val_name in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "                        metrics_df.loc[(metrics_df.LLM_NAME == llm_name) & (metrics_df.DATASET_NAME == dataset_name) & (metrics_df.CLASSIFICATION_LEVELS == levels) & (metrics_df.FILL_EVIDENCE == fill) & (metrics_df.INVERT_EVIDENCE == invert) & (metrics_df.TRUNCATED_RANKING_RETRIEVER_RESULTS == i), val_name] = rb_df_row[val_name].iloc[0]\n",
    "                        \n",
    "                    # Assertion check\n",
    "                    assert len(raw_data_df_subset) == (metrics_df_row['n_total'].iloc[0] - metrics_df_row['n_undefined_prediction'].iloc[0])\n",
    "                    \n",
    "                    n_fixed += 1\n",
    "                    "
   ],
   "id": "ed57889b6e72e33a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Additional check of correctness\n",
    "assert len(metrics_df[metrics_df.DATASET_NAME == 'climate_fever']) == n_fixed"
   ],
   "id": "d23b4270d48c0a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Write fixes to files",
   "id": "d1bfd06e85afb49d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "folder_name = '20241011-212456-UTC-fixed_climate_fever'",
   "id": "5ef3c39c8a192a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_file_path = os.path.join(folder_path_prefix, folder_name, 'raw_data.csv')\n",
    "raw_data_df.to_csv(output_file_path)"
   ],
   "id": "9743ca1744465ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_file_path = os.path.join(folder_path_prefix, folder_name, 'metrics.csv')\n",
    "metrics_df.to_csv(output_file_path)"
   ],
   "id": "a1838862c52e6769",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
